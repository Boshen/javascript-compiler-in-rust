"use strict";(self.webpackChunkjavascript_compiler_in_rust=self.webpackChunkjavascript_compiler_in_rust||[]).push([[4],{9613:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>f});var r=n(9496);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),d=c(n),f=i,m=d["".concat(s,".").concat(f)]||d[f]||p[f]||a;return n?r.createElement(m,l(l({ref:t},u),{},{components:n})):r.createElement(m,l({ref:t},u))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,l=new Array(a);l[0]=d;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:i,l[1]=o;for(var c=2;c<a;c++)l[c]=n[c];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2351:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var r=n(2081),i=(n(9496),n(9613));const a={id:"lexer",title:"Lexer"},l=void 0,o={unversionedId:"lexer",id:"lexer",title:"Lexer",description:"The lexer, also known as tokenizer or scanner, is responsible for transforming source text to tokens.",source:"@site/docs/lexer.md",sourceDirName:".",slug:"/lexer",permalink:"/javascript-compiler-in-rust/docs/lexer",draft:!1,tags:[],version:"current",frontMatter:{id:"lexer",title:"Lexer"},sidebar:"tutorialSidebar",previous:{title:"Architecture Overview",permalink:"/javascript-compiler-in-rust/docs/architecture"},next:{title:"Abstract Syntax Tree",permalink:"/javascript-compiler-in-rust/docs/ast"}},s={},c=[{value:"JavaScript",id:"javascript",level:2},{value:"Specification on Lexical Grammar",id:"specification-on-lexical-grammar",level:3},{value:"Identifiers: UTF-8 vs UTF-16",id:"identifiers-utf-8-vs-utf-16",level:3},{value:"LL(1)",id:"ll1",level:3},{value:"Re-lex",id:"re-lex",level:3},{value:"Strict Mode",id:"strict-mode",level:3},{value:"Rust Optimizations",id:"rust-optimizations",level:2},{value:"Jump Table",id:"jump-table",level:3},{value:"Unicode Identifier Start",id:"unicode-identifier-start",level:3},{value:"Small Tokens",id:"small-tokens",level:3},{value:"String Interning",id:"string-interning",level:3}],u={toc:c};function p(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"The lexer, also known as tokenizer or scanner, is responsible for transforming source text to tokens.\nThe tokens will later be consumed by the parser so we don't need to worry about whitespaces."),(0,i.kt)("p",null,"Let's start by defining a basic lexer and a single ",(0,i.kt)("inlineCode",{parentName:"p"},"+")," token."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-rust"},"use std::str::Chars;\n\nstruct Lexer<'a> {\n  chars: Chars<'a>\n}\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-rust"},"enum Token {\n  Plus,\n  Eof\n}\n")),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"Chars")," iterator is a super nice interface for working with strings,\nit will give you a ",(0,i.kt)("inlineCode",{parentName:"p"},"Option<char>")," when you call ",(0,i.kt)("inlineCode",{parentName:"p"},"chars.next()"),".\nThe only thing to be careful is that a ",(0,i.kt)("inlineCode",{parentName:"p"},"char")," is not a 0-255 ascii value,\nit is a utf8 unicode point value with the range of 0 to 0x10FFFF."),(0,i.kt)("p",null,"To convert the source text to tokens, all you need is a loop, and test the return value of each ",(0,i.kt)("inlineCode",{parentName:"p"},"chars.next()")," call."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-rust"},"fn read_next_token(&mut self) -> Token {\n  while let Some(c) = self.chars.next() {\n    match c {\n      '+' => return Token::Plus,\n      _ => {}\n    }\n  }\n  Token::Eof\n}\n")),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"javascript"},"JavaScript"),(0,i.kt)("p",null,"A lexer written in Rust is acutally really boring, it feels like writing C code\nwhere you write long chained if statements and check for each ",(0,i.kt)("inlineCode",{parentName:"p"},"char")," and then return the respective token."),(0,i.kt)("p",null,"But, the fun begins when you start writing it for JavaScript."),(0,i.kt)("h3",{id:"specification-on-lexical-grammar"},"Specification on Lexical Grammar"),(0,i.kt)("h3",{id:"identifiers-utf-8-vs-utf-16"},"Identifiers: UTF-8 vs UTF-16"),(0,i.kt)("h3",{id:"ll1"},"LL(1)"),(0,i.kt)("h3",{id:"re-lex"},"Re-lex"),(0,i.kt)("h3",{id:"strict-mode"},"Strict Mode"),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"rust-optimizations"},"Rust Optimizations"),(0,i.kt)("h3",{id:"jump-table"},"Jump Table"),(0,i.kt)("h3",{id:"unicode-identifier-start"},"Unicode Identifier Start"),(0,i.kt)("h3",{id:"small-tokens"},"Small Tokens"),(0,i.kt)("h3",{id:"string-interning"},"String Interning"))}p.isMDXComponent=!0}}]);