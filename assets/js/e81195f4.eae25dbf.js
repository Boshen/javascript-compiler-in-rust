"use strict";(self.webpackChunkjavascript_compiler_in_rust=self.webpackChunkjavascript_compiler_in_rust||[]).push([[348],{9613:(e,n,r)=>{r.d(n,{Zo:()=>p,kt:()=>m});var t=r(9496);function a(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function s(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function o(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?s(Object(r),!0).forEach((function(n){a(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):s(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function i(e,n){if(null==e)return{};var r,t,a=function(e,n){if(null==e)return{};var r,t,a={},s=Object.keys(e);for(t=0;t<s.length;t++)r=s[t],n.indexOf(r)>=0||(a[r]=e[r]);return a}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)r=s[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=t.createContext({}),c=function(e){var n=t.useContext(l),r=n;return e&&(r="function"==typeof e?e(n):o(o({},n),e)),r},p=function(e){var n=c(e.components);return t.createElement(l.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef((function(e,n){var r=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),d=c(r),m=a,f=d["".concat(l,".").concat(m)]||d[m]||u[m]||s;return r?t.createElement(f,o(o({ref:n},p),{},{components:r})):t.createElement(f,o({ref:n},p))}));function m(e,n){var r=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var s=r.length,o=new Array(s);o[0]=d;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i.mdxType="string"==typeof e?e:a,o[1]=i;for(var c=2;c<s;c++)o[c]=r[c];return t.createElement.apply(null,o)}return t.createElement.apply(null,r)}d.displayName="MDXCreateElement"},773:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var t=r(2081),a=(r(9496),r(9613));const s={id:"parser",title:"Parser"},o=void 0,i={unversionedId:"parser",id:"parser",title:"Parser",description:"The parser we are going to construct is called a recursive descent parser,",source:"@site/docs/parser.md",sourceDirName:".",slug:"/parser",permalink:"/javascript-compiler-in-rust/docs/parser",draft:!1,editUrl:"https://github.com/Boshen/javascript-compiler-in-rust/tree/main/docs/docs/parser.md",tags:[],version:"current",frontMatter:{id:"parser",title:"Parser"},sidebar:"tutorialSidebar",previous:{title:"Abstract Syntax Tree",permalink:"/javascript-compiler-in-rust/docs/ast"},next:{title:"Semantic Analysis",permalink:"/javascript-compiler-in-rust/docs/semantics_analysis"}},l={},c=[{value:"Helper functions",id:"helper-functions",level:2},{value:"Parse Functions",id:"parse-functions",level:2},{value:"Dealing with errors",id:"dealing-with-errors",level:2},{value:"Parsing Expressions",id:"parsing-expressions",level:2},{value:"Recursive Descent",id:"recursive-descent",level:3},{value:"Pratt Parsing",id:"pratt-parsing",level:3},{value:"Rust Optimizations",id:"rust-optimizations",level:2}],p={toc:c};function u(e){let{components:n,...r}=e;return(0,a.kt)("wrapper",(0,t.Z)({},p,r,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"The parser we are going to construct is called a recursive descent parser,\nit is a manual process of going down the grammar and building up the AST."),(0,a.kt)("p",null,"The parser starts simple, it holds the source code, the lexer, and the current token consumed from the lexer."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},"pub struct Parser<'a> {\n    /// Source Code\n    source: &'a str,\n\n    lexer: Lexer<'a>,\n\n    /// Current Token consumed from the lexer\n    cur_token: Token,\n\n    /// The end range of the previous token\n    prev_node_end: usize,\n}\n\nimpl<'a> Parser<'a> {\n    pub fn new(source: &'a str) -> Self {\n        Self {\n            source,\n            lexer: Lexer::new(source),\n            cur_token: Token::default(),\n        }\n    }\n\n    pub fn parse(&mut self) -> Program<'a> {\n        Ok(Program {\n            node: Node {\n                start: 0,\n                end: self.source.len(),\n            }\n            body: vec![]\n        })\n    }\n}\n")),(0,a.kt)("h2",{id:"helper-functions"},"Helper functions"),(0,a.kt)("p",null,"The current token ",(0,a.kt)("inlineCode",{parentName:"p"},"cur_token: Token")," holds the current token returned from the lexer.\nWe'll make the parser code cleaner by adding some helper functions for navigating and inspecting this token."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},"impl<'a> Parser<'a> {\n    fn start_node(&self) -> Node {\n        let token = self.cur_token();\n        Node::new(token.start, 0)\n    }\n\n    fn finish_node(&self, node: Node) -> Node {\n        Node::new(node.start, self.prev_node_end)\n    }\n\n    fn cur_token(&self) -> &Token {\n        &self.cur_token\n    }\n\n    fn cur_kind(&self) -> Kind {\n        self.cur_token.kind\n    }\n\n    /// Checks if the current index has token `Kind`\n    fn at(&self, kind: Kind) -> bool {\n        self.cur_kind() == kind\n    }\n\n    /// Advance and return true if we are at `Kind`\n    fn bump(&mut self, kind: Kind) {\n        if self.at(kind) {\n            self.advance();\n        }\n    }\n\n    /// Advance any token\n    fn bump_any(&mut self) {\n        self.advance();\n    }\n\n    /// Advance and return true if we are at `Kind`, return false otherwise\n    fn eat(&mut self, kind: Kind) -> bool {\n        if self.at(kind) {\n            self.advance();\n            return true;\n        }\n        false\n    }\n\n    /// Move to the next token\n    fn advance(&mut self) {\n        let token = self.lexer.next_token();\n        self.prev_node_end = self.cur_token.end;\n        self.cur_token = token;\n    }\n}\n")),(0,a.kt)("h2",{id:"parse-functions"},"Parse Functions"),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"DebuggerStatement")," is the most simple statement to parse, so let's try and parse it and return a valid program"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},"impl<'a> Parser<'a> {\n    pub fn parse(&mut self) -> Program {\n        let stmt = self.parse_debugger_statement();\n        let body = vec![stmt];\n        Ok(Program {\n            node: Node {\n                start: 0,\n                end: self.source.len(),\n            }\n            body,\n        })\n    }\n\n    fn parse_debugger_statement(&mut self) -> Statement {\n        let node = self.start_node();\n        self.bump_any();\n        Statement::DebuggerStatement {\n            node: self.finish_node(node),\n        }\n    }\n}\n")),(0,a.kt)("p",null,"All the other parse functions build on these primitive helper functions,\nfor example parsing the ",(0,a.kt)("inlineCode",{parentName:"p"},"while")," statement in swc:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust",metastring:"reference",reference:!0},"https://github.com/swc-project/swc/blob/554b459e26b24202f66c3c58a110b3f26bbd13cd/crates/swc_ecma_parser/src/parser/stmt.rs#L952-L970\n")),(0,a.kt)("h2",{id:"dealing-with-errors"},"Dealing with errors"),(0,a.kt)("p",null,"Quoting from the ",(0,a.kt)("a",{parentName:"p",href:"https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811"},"Dragon Book")),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Most programming language specifications do not describe how a compiler should respond to errors; error handling is left to the compiler designer.\nPlanning the error handling right from the start can both simplify the structure of a compiler and improve its handling of errors.")),(0,a.kt)("p",null,"A fully recoverable parser can construct an AST no matter what we throw at it.\nFor tools such as linter or formatter, one would wish for a fully recoverable parser so we can act on part of the program."),(0,a.kt)("p",null,"A panicking parser will abort if there is any grammar mismatch, and a partially recoverable parser will recover from deterministic grammars."),(0,a.kt)("p",null,"For example, given a grammatically incorrect while statement ",(0,a.kt)("inlineCode",{parentName:"p"},"while true {}"),", we know it is missing round brackets,\nand the only punctuation it can have are round brackets, so we can still return a valid AST and indicate its missing brackets."),(0,a.kt)("p",null,"Most JavaScript parsers out there are partially recoverable, so we'll do the same and build a partially recoverable parser."),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"The ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/rome/tools"},"Rome")," parser is a fully recoverable parser.")),(0,a.kt)("h2",{id:"parsing-expressions"},"Parsing Expressions"),(0,a.kt)("h3",{id:"recursive-descent"},"Recursive Descent"),(0,a.kt)("h3",{id:"pratt-parsing"},"Pratt Parsing"),(0,a.kt)("h2",{id:"rust-optimizations"},"Rust Optimizations"))}u.isMDXComponent=!0}}]);