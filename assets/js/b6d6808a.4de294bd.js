"use strict";(self.webpackChunkjavascript_compiler_in_rust=self.webpackChunkjavascript_compiler_in_rust||[]).push([[4],{9613:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(9496);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(n),m=a,f=d["".concat(s,".").concat(m)]||d[m]||u[m]||i;return n?r.createElement(f,o(o({ref:t},c),{},{components:n})):r.createElement(f,o({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2351:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var r=n(2081),a=(n(9496),n(9613));const i={id:"lexer",title:"Lexer"},o=void 0,l={unversionedId:"lexer",id:"lexer",title:"Lexer",description:"The lexer, also known as tokenizer or scanner, is responsible for transforming source text to tokens.",source:"@site/docs/lexer.md",sourceDirName:".",slug:"/lexer",permalink:"/javascript-compiler-in-rust/docs/lexer",draft:!1,tags:[],version:"current",frontMatter:{id:"lexer",title:"Lexer"},sidebar:"tutorialSidebar",previous:{title:"Architecture Overview",permalink:"/javascript-compiler-in-rust/docs/architecture"},next:{title:"Abstract Syntax Tree",permalink:"/javascript-compiler-in-rust/docs/ast"}},s={},p=[{value:"JavaScript",id:"javascript",level:2},{value:"Identifiers: UTF-8 vs UTF-16",id:"identifiers-utf-8-vs-utf-16",level:3},{value:"LL(1)",id:"ll1",level:3},{value:"Re-lex",id:"re-lex",level:3},{value:"Strict Mode",id:"strict-mode",level:3},{value:"Rust Optimizations",id:"rust-optimizations",level:2},{value:"Jump Table",id:"jump-table",level:3},{value:"Unicode Identifier Start",id:"unicode-identifier-start",level:3},{value:"Small Tokens",id:"small-tokens",level:3},{value:"String Interning",id:"string-interning",level:3}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"The lexer, also known as tokenizer or scanner, is responsible for transforming source text to tokens.\nThe tokens will later be consumed by the parser so we don't need to worry about whitespaces and comments in the original text."),(0,a.kt)("p",null,"Let's start simple and transform a single ",(0,a.kt)("inlineCode",{parentName:"p"},"+")," text into a token."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},"#[derive(Debug, Clone, Copy, PartialEq)]\npub struct Token {\n    /// Token Type\n    pub kind: Kind,\n\n    /// Start offset in source\n    pub start: usize,\n\n    /// End offset in source\n    pub end: usize,\n}\n\n#[derive(Debug, Clone, Copy, PartialEq)]\npub enum Kind {\n    Eof, // end of file\n    Plus,\n}\n")),(0,a.kt)("p",null,"A single ",(0,a.kt)("inlineCode",{parentName:"p"},"+")," will give you ",(0,a.kt)("inlineCode",{parentName:"p"},"[Token { kind: Kind::Plus, start: 0, end: 1 }, Token { kind: Kind::Eof, start: 1, end: 1 }]")),(0,a.kt)("p",null,"To loop through the string, we can either keep track of an index and pretend that we are writing C code,\nor we can take a look at the ",(0,a.kt)("a",{parentName:"p",href:"https://doc.rust-lang.org/std/primitive.str.html#"},"string documentation")," and find ourself a ",(0,a.kt)("inlineCode",{parentName:"p"},"Chars")," iterator to work with."),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"The ",(0,a.kt)("inlineCode",{parentName:"p"},"Chars")," iterator abstracts away the tracking index and boundary checking to make you feel really safe."),(0,a.kt)("p",{parentName:"admonition"},"It gives you an ",(0,a.kt)("inlineCode",{parentName:"p"},"Option<char>")," when you call ",(0,a.kt)("inlineCode",{parentName:"p"},"chars.next()"),".\nBut please note that a ",(0,a.kt)("inlineCode",{parentName:"p"},"char")," is not a 0-255 ascii value,\nit is a utf8 unicode point value with the range of 0 to 0x10FFFF.")),(0,a.kt)("p",null,"Let's define a starter lexer abstraction"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},"use std::str::Chars;\n\nstruct Lexer<'a> {\n    chars: Chars<'a>\n}\n")),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"The lifetime ",(0,a.kt)("inlineCode",{parentName:"p"},"'a")," here indicates the iterator has a reference to somewhere, in this case it references to a ",(0,a.kt)("inlineCode",{parentName:"p"},"&'a str"),".")),(0,a.kt)("p",null,"To convert the source text to tokens, we need a simple loop, and test the return value of each ",(0,a.kt)("inlineCode",{parentName:"p"},"chars.next()")," call."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},"impl<'a> Lexer<'a> {\n    fn read_next_token(&mut self) -> Token {\n      while let Some(c) = self.chars.next() {\n        match c {\n          '+' => return Token::Plus,\n          _ => {}\n        }\n      }\n      Token::Eof\n    }\n}\n")),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"javascript"},"JavaScript"),(0,a.kt)("p",null,"A lexer written in Rust is acutally really boring, it feels like writing C code\nwhere you write long chained if statements and check for each ",(0,a.kt)("inlineCode",{parentName:"p"},"char")," and then return the respective token."),(0,a.kt)("p",null,"But the fun begins when we start modifying it for JavaScript."),(0,a.kt)("p",null,"Let's open up the ",(0,a.kt)("a",{parentName:"p",href:"https://tc39.es/ecma262/"},"ECMAScript Language Specification")," and re-learn JavaScript."),(0,a.kt)("admonition",{type:"caution"},(0,a.kt)("p",{parentName:"admonition"},"I still remember the first time I opened up the specification and went into a corner\nand cried for like five minutes because I couldn't understand what was going on.\nSo please prepare yourself and read my ",(0,a.kt)("a",{parentName:"p",href:"/blog/ecma-spec"},"guide on reading the specification"),".")),(0,a.kt)("h3",{id:"identifiers-utf-8-vs-utf-16"},"Identifiers: UTF-8 vs UTF-16"),(0,a.kt)("h3",{id:"ll1"},"LL(1)"),(0,a.kt)("h3",{id:"re-lex"},"Re-lex"),(0,a.kt)("h3",{id:"strict-mode"},"Strict Mode"),(0,a.kt)("hr",null),(0,a.kt)("h2",{id:"rust-optimizations"},"Rust Optimizations"),(0,a.kt)("h3",{id:"jump-table"},"Jump Table"),(0,a.kt)("h3",{id:"unicode-identifier-start"},"Unicode Identifier Start"),(0,a.kt)("h3",{id:"small-tokens"},"Small Tokens"),(0,a.kt)("h3",{id:"string-interning"},"String Interning"))}u.isMDXComponent=!0}}]);